{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# The Adaptive Scientist: A Reinforcement Learning Proof-of-Concept\n"
      ],
      "metadata": {
        "id": "PLVUC7pD1I5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Environment Setup & CFD Baseline"
      ],
      "metadata": {
        "id": "5M5aSaA11Tkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install OpenFOAM using the official script.\n",
        "# This may take several minutes.\n",
        "print(\"Installing OpenFOAM and dependencies using the official script...\")\n",
        "!wget -q -O - https://dl.openfoam.org/add-debian-repo.sh | sudo bash > /dev/null 2>&1\n",
        "!sudo apt-get -y update > /dev/null 2>&1\n",
        "!sudo apt-get -y install openfoam9 > /dev/null 2>&1\n",
        "print(\"OpenFOAM installation command finished.\")\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "# Define the path to the OpenFOAM environment setup script.\n",
        "openfoam_bash_path = \"/opt/openfoam9/etc/bashrc\"\n",
        "if not os.path.exists(openfoam_bash_path):\n",
        "    print(\"\\nFATAL ERROR: OpenFOAM bashrc file not found at the expected path!\")\n",
        "    print(\"This likely means the OpenFOAM installation failed.\")\n",
        "    sys.exit(\"Halting script due to failed OpenFOAM setup.\")\n",
        "else:\n",
        "    print(f\"Using OpenFOAM bashrc at: {openfoam_bash_path}\")\n",
        "\n",
        "# --- Robust Path and Directory Management ---\n",
        "content_path = '/content'\n",
        "repo_path = os.path.join(content_path, 'PPO-PyTorch')\n",
        "\n",
        "# Always start from the content directory\n",
        "%cd {content_path}\n",
        "\n",
        "# Clone PPO Repository\n",
        "print(\"\\nCloning PPO repository...\")\n",
        "if os.path.exists(repo_path):\n",
        "    !rm -rf {repo_path}\n",
        "!git clone -q https://github.com/nikhilbarhate99/PPO-PyTorch.git\n",
        "print(f\"Repository cloned to {repo_path}.\")\n",
        "\n",
        "# --- Create the CFD Case from Scratch ---\n",
        "print(\"\\nCreating self-contained 'cavity' CFD case...\")\n",
        "case_name = \"cavity\"\n",
        "case_path = os.path.join(repo_path, case_name)\n",
        "os.makedirs(os.path.join(case_path, 'system'), exist_ok=True)\n",
        "os.makedirs(os.path.join(case_path, 'constant'), exist_ok=True)\n",
        "os.makedirs(os.path.join(case_path, '0'), exist_ok=True)\n",
        "\n",
        "# --- Define File Contents ---\n",
        "\n",
        "blockMeshDict_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; object blockMeshDict; }\n",
        "convertToMeters 1;\n",
        "vertices (\n",
        "    (0 0 0) (1 0 0) (1 1 0) (0 1 0)\n",
        "    (0 0 0.1) (1 0 0.1) (1 1 0.1) (0 1 0.1)\n",
        ");\n",
        "blocks ( hex (0 1 2 3 4 5 6 7) (20 20 1) simpleGrading (1 1 1) );\n",
        "edges ();\n",
        "boundary (\n",
        "    movingWall { type wall; faces ((3 7 6 2)); }\n",
        "    fixedWalls { type wall; faces ((0 4 7 3) (2 6 5 1) (1 5 4 0)); }\n",
        "    frontAndBack { type empty; faces ((0 3 2 1) (4 5 6 7)); }\n",
        ");\n",
        "mergePatchPairs ();\n",
        "\"\"\"\n",
        "\n",
        "controlDict_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; location \"system\"; object controlDict; }\n",
        "application     simpleFoam;\n",
        "startFrom       latestTime;\n",
        "startTime       0;\n",
        "stopAt          endTime;\n",
        "endTime         1000;\n",
        "deltaT          1;\n",
        "writeControl    timeStep;\n",
        "writeInterval   50;\n",
        "purgeWrite      0;\n",
        "writeFormat     ascii;\n",
        "writePrecision  6;\n",
        "writeCompression off;\n",
        "timeFormat      general;\n",
        "timePrecision   6;\n",
        "runTimeModifiable true;\n",
        "\"\"\"\n",
        "\n",
        "fvSchemes_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; location \"system\"; object fvSchemes; }\n",
        "ddtSchemes { default steadyState; }\n",
        "gradSchemes { default Gauss linear; }\n",
        "divSchemes {\n",
        "    default         none;\n",
        "    div(phi,U)      Gauss upwind;\n",
        "    div((nuEff*dev2(T(grad(U))))) Gauss linear;\n",
        "}\n",
        "laplacianSchemes { default Gauss linear orthogonal; }\n",
        "interpolationSchemes { default linear; }\n",
        "snGradSchemes { default orthogonal; }\n",
        "\"\"\"\n",
        "\n",
        "fvSolution_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; location \"system\"; object fvSolution; }\n",
        "solvers {\n",
        "    p { solver PCG; preconditioner DIC; tolerance 1e-06; relTol 0.1; }\n",
        "    U { solver PBiCGStab; preconditioner DILU; tolerance 1e-08; relTol 0; }\n",
        "}\n",
        "SIMPLE {\n",
        "    nNonOrthogonalCorrectors 0;\n",
        "    pRefCell 0;\n",
        "    pRefValue 0;\n",
        "}\n",
        "relaxationFactors {\n",
        "    fields { p 0.3; }\n",
        "    equations { U 0.7; }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "transportProperties_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; location \"constant\"; object transportProperties; }\n",
        "transportModel Newtonian;\n",
        "nu [0 2 -1 0 0 0 0] 0.01;\n",
        "\"\"\"\n",
        "\n",
        "momentumTransport_content = \"\"\"\n",
        "FoamFile { format ascii; class dictionary; location \"constant\"; object momentumTransport; }\n",
        "simulationType laminar;\n",
        "\"\"\"\n",
        "\n",
        "U_content = \"\"\"\n",
        "FoamFile { format ascii; class volVectorField; object U; }\n",
        "dimensions [0 1 -1 0 0 0 0];\n",
        "internalField uniform (0 0 0);\n",
        "boundaryField {\n",
        "    movingWall { type fixedValue; value uniform (1 0 0); }\n",
        "    fixedWalls { type noSlip; }\n",
        "    frontAndBack { type empty; }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "p_content = \"\"\"\n",
        "FoamFile { format ascii; class volScalarField; object p; }\n",
        "dimensions [0 2 -2 0 0 0 0];\n",
        "internalField uniform 0;\n",
        "boundaryField {\n",
        "    movingWall { type zeroGradient; }\n",
        "    fixedWalls { type zeroGradient; }\n",
        "    frontAndBack { type empty; }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Write files ---\n",
        "with open(os.path.join(case_path, \"system\", \"blockMeshDict\"), 'w') as f: f.write(blockMeshDict_content)\n",
        "with open(os.path.join(case_path, \"system\", \"controlDict\"), 'w') as f: f.write(controlDict_content)\n",
        "with open(os.path.join(case_path, \"system\", \"fvSchemes\"), 'w') as f: f.write(fvSchemes_content)\n",
        "with open(os.path.join(case_path, \"system\", \"fvSolution\"), 'w') as f: f.write(fvSolution_content)\n",
        "with open(os.path.join(case_path, \"constant\", \"transportProperties\"), 'w') as f: f.write(transportProperties_content)\n",
        "with open(os.path.join(case_path, \"constant\", \"momentumTransport\"), 'w') as f: f.write(momentumTransport_content)\n",
        "with open(os.path.join(case_path, \"0\", \"U\"), 'w') as f: f.write(U_content)\n",
        "with open(os.path.join(case_path, \"0\", \"p\"), 'w') as f: f.write(p_content)\n",
        "\n",
        "print(\"Case files created successfully.\")\n",
        "\n",
        "\n",
        "# --- Function to run OpenFOAM commands safely ---\n",
        "def run_foam_command(command, log_filename):\n",
        "    full_cmd = f\"source {openfoam_bash_path} && {command}\"\n",
        "    try:\n",
        "        with open(os.path.join(case_path, log_filename), 'w') as log_file:\n",
        "            subprocess.run(full_cmd, shell=True, check=True, executable='/bin/bash', cwd=case_path, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        print(f\"{command} successful.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"\\nFATAL ERROR: {command} failed.\")\n",
        "        log_file_path = os.path.join(case_path, log_filename)\n",
        "        if os.path.exists(log_file_path):\n",
        "            with open(log_file_path, \"r\") as f:\n",
        "                print(f\"--- {log_filename} ---\")\n",
        "                print(f.read())\n",
        "        sys.exit(f\"Halting script due to {command} error.\")\n",
        "\n",
        "# --- Run Baseline Simulation ---\n",
        "run_foam_command(\"blockMesh\", \"log.blockMesh\")\n",
        "run_foam_command(\"simpleFoam\", \"log.baseline\")\n",
        "\n",
        "print(\"Baseline simulation complete.\")\n",
        "\n",
        "# Establish Baseline\n",
        "print(\"\\n================================================================\")\n",
        "print(\"ACTION REQUIRED: Establish Baseline Performance\")\n",
        "print(f\"From the file browser on the left, navigate to '{case_path}' and open the 'log.baseline' file.\")\n",
        "print(\"Scroll to the end and note the number of 'Time =' steps. This is the iteration count your RL agent will try to beat.\")\n",
        "print(\"================================================================\\n\")\n",
        "\n",
        "# Change to the repo directory for the subsequent cells\n",
        "%cd {repo_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs4pOMzsYC5o",
        "outputId": "6f6ab7dc-16d4-4ddd-d611-4dc8b753356c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing OpenFOAM and dependencies using the official script...\n",
            "OpenFOAM installation command finished.\n",
            "Using OpenFOAM bashrc at: /opt/openfoam9/etc/bashrc\n",
            "/content\n",
            "\n",
            "Cloning PPO repository...\n",
            "Repository cloned to /content/PPO-PyTorch.\n",
            "\n",
            "Creating self-contained 'cavity' CFD case...\n",
            "Case files created successfully.\n",
            "blockMesh successful.\n",
            "simpleFoam successful.\n",
            "Baseline simulation complete.\n",
            "\n",
            "================================================================\n",
            "ACTION REQUIRED: Establish Baseline Performance\n",
            "From the file browser on the left, navigate to '/content/PPO-PyTorch/cavity' and open the 'log.baseline' file.\n",
            "Scroll to the end and note the number of 'Time =' steps. This is the iteration count your RL agent will try to beat.\n",
            "================================================================\n",
            "\n",
            "/content/PPO-PyTorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ.get('HOME'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ZX2vDlAmnl",
        "outputId": "96abf079-dc22-49ca-8a98-fbb4689d5bf7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dependencies and Custom Environment"
      ],
      "metadata": {
        "id": "21oETuv61nmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "import time\n",
        "import glob # Import glob to find time directories\n",
        "\n",
        "print(\"\\nLibraries imported. Defining the custom CFD environment...\")\n",
        "\n",
        "class CFDEnv(gym.Env):\n",
        "    def __init__(self, openfoam_bash):\n",
        "        super(CFDEnv, self).__init__()\n",
        "        self.openfoam_bash = openfoam_bash\n",
        "        self.repo_path = '/content/PPO-PyTorch'\n",
        "        self.base_case_path = os.path.join(self.repo_path, \"cavity\")\n",
        "\n",
        "        # Unique instance path to avoid conflicts\n",
        "        self.instance_id = f\"instance_{time.time()}_{np.random.randint(1000)}\"\n",
        "        self.case_path = os.path.join(\"/tmp\", self.instance_id)\n",
        "\n",
        "        # File paths\n",
        "        self.fv_solution_path = os.path.join(self.case_path, 'system', 'fvSolution')\n",
        "        self.control_dict_path = os.path.join(self.case_path, 'system', 'controlDict')\n",
        "        self.log_path = os.path.join(self.case_path, 'log.simpleFoam')\n",
        "\n",
        "        self.action_space = spaces.Box(low=0.1, high=1.0, shape=(2,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(10,), dtype=np.float32)\n",
        "\n",
        "        self.residual_history = np.zeros(10, dtype=np.float32)\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 100 # Max iterations per episode\n",
        "\n",
        "    def _get_latest_time(self):\n",
        "        \"\"\"Finds the latest time step directory in the case.\"\"\"\n",
        "        time_dirs = glob.glob(os.path.join(self.case_path, '[0-9]*'))\n",
        "        if not time_dirs:\n",
        "            return 0\n",
        "        return max([int(os.path.basename(d)) for d in time_dirs if os.path.basename(d).isdigit()])\n",
        "\n",
        "    def _set_control_dict(self, start_time, end_time):\n",
        "        \"\"\"Dynamically sets the start and end time in controlDict.\"\"\"\n",
        "        with open(self.control_dict_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        content = re.sub(r'startFrom\\s+\\w+;', f'startFrom\\tstartTime;', content)\n",
        "        content = re.sub(r'startTime\\s+[\\d\\.]+;', f'startTime\\t{start_time};', content)\n",
        "        content = re.sub(r'endTime\\s+[\\d\\.]+;', f'endTime\\t{end_time};', content)\n",
        "        with open(self.control_dict_path, 'w') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    def _run_openfoam_command(self, command):\n",
        "        \"\"\"Runs an OpenFOAM command.\"\"\"\n",
        "        full_cmd = f\"source {self.openfoam_bash} && {command}\"\n",
        "        try:\n",
        "            # Use a single log file, overwriting it for each command\n",
        "            with open(self.log_path, 'w') as log_file:\n",
        "                subprocess.run(\n",
        "                    full_cmd, shell=True, check=True, cwd=self.case_path,\n",
        "                    executable='/bin/bash', stdout=log_file, stderr=subprocess.STDOUT\n",
        "                )\n",
        "            return True\n",
        "        except subprocess.CalledProcessError:\n",
        "            return False\n",
        "\n",
        "    def _parse_residuals(self):\n",
        "        \"\"\"Parses the final residuals from the log file.\"\"\"\n",
        "        residuals = {'Ux': None, 'p': None}\n",
        "        try:\n",
        "            with open(self.log_path, 'r') as f:\n",
        "                log_content = f.read()\n",
        "            # Find the last occurrence of 'Solving for' blocks\n",
        "            u_match = re.findall(r'Solving for Ux.*Initial residual = ([\\d.e-]+)', log_content)\n",
        "            p_match = re.findall(r'Solving for p.*Initial residual = ([\\d.e-]+)', log_content)\n",
        "            if u_match: residuals['Ux'] = float(u_match[-1])\n",
        "            if p_match: residuals['p'] = float(p_match[-1])\n",
        "        except (FileNotFoundError, IndexError):\n",
        "            return None, True\n",
        "        if residuals['Ux'] is None or residuals['p'] is None or np.isnan(residuals['Ux']) or np.isnan(residuals['p']):\n",
        "            return None, True # Diverged if residuals are bad\n",
        "        return np.array([residuals['Ux'], residuals['p']]), False\n",
        "\n",
        "    def _update_relaxation_factors(self, action):\n",
        "        \"\"\"Updates relaxation factors in fvSolution.\"\"\"\n",
        "        u_urf, p_urf = np.clip(action[0], 0.1, 1.0), np.clip(action[1], 0.1, 1.0)\n",
        "        with open(self.fv_solution_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        content = re.sub(r'(U\\s+)[0-9\\.]+', fr'\\g<1>{u_urf:.3f}', content)\n",
        "        content = re.sub(r'(p\\s+)[0-9\\.]+', fr'\\g<1>{p_urf:.3f}', content)\n",
        "        with open(self.fv_solution_path, 'w') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        self._update_relaxation_factors(action)\n",
        "\n",
        "        # Get current time and set controlDict to run for ONE step\n",
        "        current_time = self._get_latest_time()\n",
        "        self._set_control_dict(start_time=current_time, end_time=current_time + 1)\n",
        "\n",
        "        command_successful = self._run_openfoam_command('simpleFoam')\n",
        "        new_residuals, diverged = (None, True) if not command_successful else self._parse_residuals()\n",
        "\n",
        "        info, terminated, truncated = {}, False, False\n",
        "        if diverged:\n",
        "            reward = -200.0\n",
        "            terminated = True\n",
        "        else:\n",
        "            # Reward is negative log of pressure residual (we want to minimize it)\n",
        "            reward = -np.log10(new_residuals[1] + 1e-10)\n",
        "            self.residual_history = np.roll(self.residual_history, 2)\n",
        "            self.residual_history[0:2] = new_residuals\n",
        "\n",
        "            # Check for convergence\n",
        "            if new_residuals[1] < 1e-5:\n",
        "                reward += 100.0 # Large bonus for converging\n",
        "                terminated = True\n",
        "\n",
        "        if self.step_count >= self.max_steps:\n",
        "             truncated = True\n",
        "\n",
        "        return self.residual_history, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.step_count = 0\n",
        "        self.residual_history.fill(0)\n",
        "\n",
        "        # Create a unique, clean case directory for this episode\n",
        "        if os.path.exists(self.case_path):\n",
        "            shutil.rmtree(self.case_path)\n",
        "        shutil.copytree(self.base_case_path, self.case_path)\n",
        "\n",
        "        # We need a meshed case but don't run the solver here.\n",
        "        # The first step will handle the first iteration.\n",
        "        self._run_openfoam_command('blockMesh')\n",
        "\n",
        "        # The initial state is a vector of zeros, as no simulation has run.\n",
        "        info = {}\n",
        "        return self.residual_history, info\n",
        "\n",
        "    def close(self):\n",
        "        if os.path.exists(self.case_path):\n",
        "            shutil.rmtree(self.case_path)\n",
        "\n",
        "print(\"CFDEnv class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm0E54hEqa5b",
        "outputId": "d0f36006-82d0-449b-b927-e8734b0ac859"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Libraries imported. Defining the custom CFD environment...\n",
            "CFDEnv class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PPO Agent Implementation"
      ],
      "metadata": {
        "id": "dwI44tUU16El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.action_dim = action_dim\n",
        "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n",
        "\n",
        "        # actor\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Tanh() # Use Sigmoid to ensure output is between 0 and 1\n",
        "        )\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        # Clip action to be between 0 and 1\n",
        "        action = torch.clamp(action, 0, 1)\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_mean = self.actor(state)\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        if self.action_dim == 1:\n",
        "            action = action.reshape(-1, self.action_dim)\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.6):\n",
        "        self.action_std = action_std_init\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        self.buffer = RolloutBuffer()\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std_init)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "        ])\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        self.action_std = new_action_std\n",
        "        self.policy.set_action_std(new_action_std)\n",
        "        self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        self.action_std = self.action_std - action_std_decay_rate\n",
        "        self.action_std = round(self.action_std, 4)\n",
        "        if (self.action_std <= min_action_std):\n",
        "            self.action_std = min_action_std\n",
        "        self.set_action_std(self.action_std)\n",
        "\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state)\n",
        "            action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "        # The action from the policy is now in [-1, 1].\n",
        "        # First, scale it to [0, 1]: (action + 1) / 2\n",
        "        # Then, scale it to [0.1, 1.0] for the environment: ... * 0.9 + 0.1\n",
        "        env_action = ((action + 1) / 2) * 0.9 + 0.1\n",
        "\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action) # Store original action [-1, 1]\n",
        "        self.buffer.logprobs.append(action_logprob)\n",
        "        self.buffer.state_values.append(state_val)\n",
        "\n",
        "        return env_action.numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach()\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding the Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.buffer.clear()\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "print(\"PPO agent classes defined.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7EON1wsZIYV",
        "outputId": "8398deab-af5f-4801-a9e4-8a8c79881aa4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO agent classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Training the Agent"
      ],
      "metadata": {
        "id": "IrlEbpQ12W6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the custom environment and the PPO agent, then run the\n",
        "# main training loop with parameters adjusted for the slow CFD environment.\n",
        "\n",
        "\n",
        "print(\"\\nStarting training setup...\")\n",
        "\n",
        "################## Hyperparameters ##################\n",
        "# Using a very small training budget for this proof-of-concept.\n",
        "# A full run might take over an hour.\n",
        "max_training_timesteps = 5000   # REDUCED for quick POC\n",
        "update_timestep = 100           # update policy every n timesteps\n",
        "K_epochs = 20                   # update policy for K epochs\n",
        "eps_clip = 0.2                  # clip parameter for PPO\n",
        "gamma = 0.99                    # discount factor\n",
        "lr_actor = 0.0001               # learning rate for actor\n",
        "lr_critic = 0.001               # learning rate for critic\n",
        "action_std = 0.1                # starting std for action distribution\n",
        "#####################################################\n",
        "\n",
        "# Create environment, passing the verified OpenFOAM path\n",
        "env = CFDEnv(openfoam_bash=openfoam_bash_path)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "# Create PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)\n",
        "\n",
        "print(f\"State Dimensions: {state_dim}\")\n",
        "print(f\"Action Dimensions: {action_dim}\")\n",
        "print(\"PPO Agent created. Starting training loop...\")\n",
        "print(\"This will take a while, please be patient...\")\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "while time_step <= max_training_timesteps:\n",
        "    state, _ = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, env.max_steps + 2):\n",
        "        # Select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # Saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(terminated or truncated)\n",
        "\n",
        "        time_step += 1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    i_episode += 1\n",
        "    # Only print episode summary every 10 episodes to reduce log spam\n",
        "    if i_episode % 10 == 0:\n",
        "        print(f\"Episode: {i_episode}, Timesteps: {time_step}, Reward: {current_ep_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "print(f\"\\nTraining finished in {time.time() - start_time:.2f} seconds.\")\n",
        "ppo_agent.save(\"PPO_CFDEnv.pth\")\n",
        "print(\"Model saved as PPO_CFDEnv.pth\")\n"
      ],
      "metadata": {
        "id": "sc72GRiG00Ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc81ac98-8605-4297-b6ff-436186ba7a9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training setup...\n",
            "State Dimensions: 10\n",
            "Action Dimensions: 2\n",
            "PPO Agent created. Starting training loop...\n",
            "This will take a while, please be patient...\n",
            "Episode: 10, Timesteps: 1000, Reward: 143.97\n",
            "Episode: 20, Timesteps: 2000, Reward: 150.96\n",
            "Episode: 30, Timesteps: 3000, Reward: 162.85\n",
            "Episode: 40, Timesteps: 4000, Reward: 148.86\n",
            "Episode: 50, Timesteps: 5000, Reward: 151.55\n",
            "\n",
            "Training finished in 1554.48 seconds.\n",
            "Model saved as PPO_CFDEnv.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Evaluation"
      ],
      "metadata": {
        "id": "zpwRVHzF2cCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the trained agent and run it for one full episode to observe its\n",
        "# learned policy and compare its performance against the baseline.\n",
        "\n",
        "\n",
        "print(\"\\nStarting evaluation...\")\n",
        "\n",
        "# Create a new environment for evaluation\n",
        "eval_env = CFDEnv(openfoam_bash=openfoam_bash_path)\n",
        "# Load the trained agent\n",
        "eval_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)\n",
        "try:\n",
        "    eval_agent.load(\"PPO_CFDEnv.pth\")\n",
        "    print(\"Loaded trained model for evaluation.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Could not find trained model 'PPO_CFDEnv.pth'. Evaluation will use an untrained agent.\")\n",
        "\n",
        "\n",
        "state, _ = eval_env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "eval_step = 0\n",
        "\n",
        "print(\"\\n| Step |      Action (U, p)      |  Reward  | Converged/Done |\")\n",
        "print(\"|------|-------------------------|----------|----------------|\")\n",
        "\n",
        "while not done and eval_step < eval_env.max_steps:\n",
        "    eval_step += 1\n",
        "    # Use the deterministic action for evaluation by taking the mean\n",
        "    state_tensor = torch.FloatTensor(state)\n",
        "    with torch.no_grad():\n",
        "        # Action is [0,1], scale it to [0.1, 1.0] for env\n",
        "        action_raw = eval_agent.policy_old.actor(state_tensor).numpy()\n",
        "        action = action_raw * 0.9 + 0.1\n",
        "\n",
        "    state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "    total_reward += reward\n",
        "    done = terminated or truncated\n",
        "\n",
        "    action_str = f\"({action[0]:.3f}, {action[1]:.3f})\"\n",
        "    print(f\"| {eval_step:<4} | {action_str:<23} | {reward:<8.2f} | {str(done):<14} |\")\n",
        "\n",
        "print(\"\\n====================== Evaluation Summary ======================\")\n",
        "print(f\"The agent took {eval_step} steps to solve the environment.\")\n",
        "print(f\"Total reward accumulated: {total_reward:.2f}\")\n",
        "print(\"\\nCompare the number of steps above with the baseline number you recorded from 'log.baseline'.\")\n",
        "print(\"Even a small improvement demonstrates a successful proof-of-concept!\")\n",
        "print(\"================================================================\")\n",
        "\n",
        "eval_env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "uqoxv3-9020a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803a6995-5f53-4f3f-db44-56bdfdd46c2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting evaluation...\n",
            "Loaded trained model for evaluation.\n",
            "\n",
            "| Step |      Action (U, p)      |  Reward  | Converged/Done |\n",
            "|------|-------------------------|----------|----------------|\n",
            "| 1    | (0.193, 0.270)          | 0.42     | False          |\n",
            "| 2    | (0.182, 0.276)          | 0.40     | False          |\n",
            "| 3    | (0.169, 0.276)          | 0.37     | False          |\n",
            "| 4    | (0.132, 0.255)          | 0.30     | False          |\n",
            "| 5    | (0.101, 0.253)          | 0.23     | False          |\n",
            "| 6    | (0.067, 0.251)          | 0.23     | False          |\n",
            "| 7    | (0.054, 0.246)          | 0.23     | False          |\n",
            "| 8    | (0.040, 0.241)          | 0.23     | False          |\n",
            "| 9    | (0.030, 0.240)          | 0.23     | False          |\n",
            "| 10   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 11   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 12   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 13   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 14   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 15   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 16   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 17   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 18   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 19   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 20   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 21   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 22   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 23   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 24   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 25   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 26   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 27   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 28   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 29   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 30   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 31   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 32   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 33   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 34   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 35   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 36   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 37   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 38   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 39   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 40   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 41   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 42   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 43   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 44   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 45   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 46   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 47   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 48   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 49   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 50   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 51   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 52   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 53   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 54   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 55   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 56   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 57   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 58   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 59   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 60   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 61   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 62   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 63   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 64   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 65   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 66   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 67   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 68   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 69   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 70   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 71   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 72   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 73   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 74   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 75   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 76   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 77   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 78   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 79   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 80   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 81   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 82   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 83   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 84   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 85   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 86   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 87   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 88   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 89   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 90   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 91   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 92   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 93   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 94   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 95   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 96   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 97   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 98   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 99   | (0.025, 0.239)          | 0.23     | False          |\n",
            "| 100  | (0.025, 0.239)          | 0.23     | True           |\n",
            "\n",
            "====================== Evaluation Summary ======================\n",
            "The agent took 100 steps to solve the environment.\n",
            "Total reward accumulated: 23.40\n",
            "\n",
            "Compare the number of steps above with the baseline number you recorded from 'log.baseline'.\n",
            "Even a small improvement demonstrates a successful proof-of-concept!\n",
            "================================================================\n"
          ]
        }
      ]
    }
  ]
}